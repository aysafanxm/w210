{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pd.read_csv('./data/text_train_deps.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>host_about</th>\n",
       "      <th>house_rules</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>notes</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125045</th>\n",
       "      <td>125025.0</td>\n",
       "      <td>This master bedroom with a private bathroom an...</td>\n",
       "      <td>Im a 28 year old engineer, national park enthu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This master bedroom with a private bathroom an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                        description  \\\n",
       "125045    125025.0  This master bedroom with a private bathroom an...   \n",
       "\n",
       "                                               host_about house_rules  \\\n",
       "125045  Im a 28 year old engineer, national park enthu...         NaN   \n",
       "\n",
       "       neighborhood_overview notes  \\\n",
       "125045                   NaN   NaN   \n",
       "\n",
       "                                                  summary  \n",
       "125045  This master bedroom with a private bathroom an...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c15d37b1f4b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_entities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0munique_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#combines vocabulary from both entity files\n",
    "unique_entities = []\n",
    "for line in open('./data/text_train_deps.csv').readlines():\n",
    "    word = line.split(',')\n",
    "    for each in word:\n",
    "        if each.lower() not in unique_entities:\n",
    "            unique_entities.append(each.lower())\n",
    "for line in open('entity_test_deps.csv').readlines():\n",
    "    word = line.split(',')\n",
    "    for each in word:\n",
    "        if each.lower() not in unique_entities:\n",
    "            unique_entities.append(each.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####converting entities to actual strings####\n",
    "str_unique_entities = []\n",
    "for each in unique_entities:\n",
    "    str_unique_entities.append(str(each.strip('\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pulling tokenized sentences from both csv\n",
    "with open('text_train_deps.csv', 'r') as f:\n",
    "    entity_test_deps = []\n",
    "    sents = []\n",
    "    heading = True\n",
    "    for line in f:\n",
    "        if heading == True:\n",
    "            heading = False\n",
    "        else:\n",
    "            sentences = nltk.sent_tokenize(line.decode('ascii', 'ignore'))\n",
    "            sents.append(sentences)\n",
    "\n",
    "with open('text_test_deps.csv', 'r') as f:\n",
    "    entity_test_deps = []\n",
    "    tok_sents = []\n",
    "    heading = True\n",
    "    for line in f:\n",
    "        if heading == True:\n",
    "            heading = False\n",
    "        else:\n",
    "            sentences = nltk.sent_tokenize(line.decode('ascii', 'ignore'))\n",
    "            sents.append(sentences)\n",
    "            tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "            tok_sents.append(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting tf and word count\n",
    "word_count = 0\n",
    "unique_entity_list = {}\n",
    "for count in range(len(sents)):\n",
    "    if len(sents[count]) > 0:\n",
    "        sentence = sents[count][0].split()\n",
    "        for word in sentence:\n",
    "            word_count += 1        \n",
    "        #counts the frequency of entities in corpus\n",
    "        for word in str_unique_entities:\n",
    "            if word in sents[count][0].lower() and word != '':\n",
    "                if word not in stop_words:\n",
    "                    if word in unique_entity_list:\n",
    "                        unique_entity_list[word] += 1\n",
    "                    else:\n",
    "                        unique_entity_list[word] = 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fat buddha', 'microwave coffee', 'vast windows', 'hip potrero hill location', 'busta rhymes', 'hbo one', 'italytown', 'remodel cozy', 'san francisco school', 'mcclaren', 'scottsdale', 'asian market', 'oracleopenworld', 'helsinki', 'bean bag', 'divisidero street', 'bitcoin', 'visitacion valley', 'bowls', 'zynga hq', 'kenny rogers', 'metallica', 'cozy san francisco home', 'kashmir', 'eurecom ecole', 'lego', 'aqua surf shop', 'business insider', 'shared bath available', 'hottub', 'golden gate bridge and golden gate park', 'sparks food truck', 'norcal', 'art history', 'boathouse', 'sunny victorian condo getaway', 'neil diamond', 'starbucks coffee', 'california trolley line', 'downtown civic center', 'dog alert', 'antioch college', 'deyoung art', 'parley', 'zico', 'dave', 'optional twin', 'dogptch', 'new china town', 'late check', 'fairmont hotel', 'top floor view flat', 'slovenia', 'serena williams', 'channel mission bay', 'french bulldog', 'spark social', 'aspen', 'media sales', 'seaside district', 'textile design', 'destiny', 'dvr garage', 'eleuthera', 'meghan', 'hypoallergenic', 'aziza', 'neighborhood bakehouse', 'bathroom kitchen full', 'pacific ocean breakfast', 'awesome area', 'weekends for', 'master bd', 'safety of', 'art institute', 'marcel patulacci', 'cacophony', 'turret', 'amazing french', 'noise please', 'sf all', 'chocolate lab', 'portrero', 'political science', 'dove', 'between bush', 'broadway strip', 'northpoint apartments', 'openideo', 'african grey', 'del fina', 'possibility', 'grandma', 'westlake', 'fairy building', 'guest loft', 'monterey heights', 'financial center', 'parking space for', 'lous caf', 'san franciscos russian hill', 'assistant', 'papalote', 'richmond bridge', 'afissos', 'clayton', 'bakehouse', 'mission concourse', 'blowfish', 'victorian main house', 'leavenworth st', 'hardly strictly bluegrass festival', 'sfsuf', 'ancorage', 'mobb deep', 'clay', 'toto', 'nicole', 'jonathan', 'fashion student', 'summer season', 'powell cable car muni', 'arched', 'gazelle palace', 'golden gate park beach judahlicious public transportation', 'cpa', 'weekly discount', 'waive', 'granite counter tops', 'gorgeous city view', 'fulton st', 'bart high', 'adios dog walking', 'amplified', 'ladakh', 'cancer research', 'north waterfront location', 'squid', 'sweetness', 'mexican grill', 'yoga tree', 'wifi password', 'rhea', 'ambience', 'hong kong/china', 'appletv', 'rec center', 'washington state', 'tranportacion', 'dreamy castro st', 'bocce', 'sunny side travel house', 'allllll', 'super markets', 'full tub', 'large flat screen', 'tiled steps', 'embarcadero area', 'work weekdays', 'lock box', 'southern parts', 'rules please', 'southern freeway', 'fleetweek', 'nordstrom rack', 'northwestern', 'martha stewart', 'bad seed', 'grandma marie', 'qeenbed', 'freeh full', 'lovejoy', 'comcast cable', 'read parking', 'dully', 'south beach san francisco', 'shorter', 'film festivals', 'donald trump', 'green chili kitchen', 'bed house', 'promo for', 'castro enjoy', 'lock front door', 'serengeti', 'washlet', 'melbourne', 'sepultura', 'public relations', 'lyon st steps', 'rodolph', 'jcc', 'cheetos', 'general motor', 'double sinks', 'dogpatch boulders', 'volkswagen', 'clayton street', 'bart sleeps', 'sharply', 'fashion blogger', 'cole valley district', 'rollaway bed', 'gym rooftop walkable', 'vsauce', 'and soap', 'dizzy gillespie', 'bi rite', 'feng shui', 'whales', 'toto washlet', 'strictly bluegrass festival', 'walk ggp', 'rockstar', 'sf private room', 'newly remodelled', 'avenue tiled steps', 'st. francis woods', 'heated pool', 'large sunny victorian', 'shared area', 'powll cable', 'embarcadero kitchen available', 'virginia tech', 'creative loft', 'discovery museum', 'bart bus', 'airbnb pictures', 'star trek', 'tantrum', 'powell stations', 'pugs', 'barrel head', 'unique luxury building', 'tom wolfe', 'tupac', 'android', 'guest ready', 'futurama', 'conference room', 'powell st cable', 'password', 'harry belafonte', 'artmar hotel', 'gold gate park', 'hotel north beach', 'hawthorne', 'frameline', 'green chile', 'customer success', 'pink floyd', 'beauty rest', 'parnassus heights campus', 'speedway meadow', 'lands ends', 'tides', 'coin op', 'electric sheep', 'downtown north beach', 'san francisco magazine', 'macondray lane', 'victoria falls', 'golden gate park plentiful', 'sf art institute', 'undercover', 'str000075', 'upper noe valley', 'oscar peterson', 'happy new year', 'delores park', 'downtown hotel', 'computer desk', 'cozy castro', 'patio garden', 'yara', 'mosconi', 'vesuvio cafe', 'hd channels', 'airbmb', 'duxiana king', 'unique guest suite', 'cameras', 'houzz', 'public works', 'timothy', 'lower ground floor kitchenless guest master suite', 'simple simple simple', 'bean bag cafe', 'blu ray', 'san francisco state university great', 'netflix hulu', 'north bay', 'south east asia', 'california as', 'peter jennings', 'little gem', 'golden gate park museums', 'free gym', 'fvf online', 'applied', 'star donuts', 'outdoorsman', 'thursdays', 'hip hop dancer', 'bunny', 'kyle', 'san francisco very', 'bayshore station', 'bose speaker', 'sf large', 'needles', 'sorry no kitchen privileges', 'evidence', 'jono', 'sweeping bay view', 'blowfi', 'papalote mexican grill', 'truly welcome', 'trendy restaurants', 'bravo', 'thinker', 'italian furnished home', 'product development', 'historic fort mason', 'blue bottle caf', 'paragon', 'italians', 'entire studio', 'brand new one bedroom', 'artistry', 'fly trap', 'willow', 'front gate key', 'no heavy cooking', 'check calendar', 'urban farmstead', 'photography studio', 'parking lot', 'egg merchantile', 'natural light queen sized bed full size sofa bed all new furniture close', 'reykjavik', 'lhc', 'english tudor', 'middle bedroom', 'san francisco wire artist', 'broadway music', 'kitchen weiner', 'mobile app design', 'spark social sf', 'registered host', 'muni rail', 'article', 'palomino', 'coffeemaker', 'quite hours', 'oliver', 'uc san francisco', 'ocean beach district', 'st. francis wood', 'zimride', 'speedway meadows', 'fillmore streets', 'san francisco lic', 'ord street', 'north american', 'sciences walk score', 'new yorkers', 'north beach play', 'bayside vista', 'sweet maple', 'golden gate bridge enjoy san francisco', 'lower polk', 'englisch', 'sutter room', 'four barrell', 'all sports', 'modern north beach condo', 'nutrition consultant', 'uc berkely', 'chambre', 'bay bridges', 'mathematics', 'barrell', 'american popular standards', 'duxiana', 'monty', 'modern bohemian', 'garage or', 'simmons beauty rest', 'kimi', 'calista', 'bobal', 'evian', 'sushi bar', 'liaison', 'pianofight', 'saru sushi bar', 'location beautiful', 'filbert street cable car', 'mcdonalds', 'westfield shopping mall', 'nordstrom', 'civic center station', 'designer digs', 'truste', 'nicks crispy taco', 'short term registry', 'rosie hienz', 'judahlicious', 'kitchen kitchen', 'giants baseball', 'ferry bldg marketplace', 'physics', 'superbowl village', 'microwave bathroom', 'burkes', 'listed for', 'fin dist', 'metropolitan museum', 'branson', 'high rise condo', 'mangiare', 'gestalt', 'de young museum tower', 'laguna st', 'soiled', 'great marina', 'fleet', 'half mile', 'vesuvio', 'st. francis diner', 'wifi large', 'upper market san franciscos', 'south east', 'new professional bayview development', 'yerba buena gadens', 'cozy neighborhood gem', 'san francisco sites', 'mission dolores church', 'pierre', 'paradise score', 'steak frites', 'franco', 'beginner surf', 'bossa', 'use be respectful of others observe', 'garden townhome', 'demand water heater', 'der schwarm', 'female only', 'pretty sweet', 'wine cabinet', 'truffle', 'phil coffee', 'giants games', 'local permit', 'merlock', 'clarendon', 'somas', 'bose sound system', 'holidays perched', 'redwood forest', 'baywiew', 'kenny', 'not to be', 'johnny bravo', 'worldmark timeshare condo', 'xfinit', 'guatemalan', 'policies sf', 'mary ann', 'bedsheets', 'lake geneva', 'moscony convention center', 'castros', 'apple products', 'glenn park.', 'siena', 'twin peaks district', 'pribado', 'cafe table', 'portrero hill', 'downtown bathroom', 'stowe lake', 'small deck', 'absolutely no parties', 'donald kim', 'golden pate', 'rainbow room', 'massachusetts', 'edward rutherfurd', 'flickr', 'nutritious', 'dawson place', 'la fromagerie', 'saru', 'username', 'single room occupancy', 'tamper', 'seal rock', 'south noe', 'stone stown shopping center', 'due to sf', 'utah', 'soma condo', 'ocean beach great', 'twitter', 'sharon meadows', 'andre and simon', 'utility', 'dream on', 'technician', 'crepe house', 'elaine', 'millennium tower', 'portrero hills', 'nona', 'gorgonzola', 'wharf take', 'no illegal', 'bdrm share', 'or late check out', 'south beach park', 'near freeway', 'gordon lightfood', 'bay area discovery museum', 'golds', 'modern penthouse', 'troyes', 'piano jazz', 'pohova county', 'fantastic bay', 'senior technical recruiter', 'smallunique', 'northern virginia', 'navy', 'sf richmond', 'ingnieurs', 'traditional chinese', 'jonathan kim', 'st. francis woo', 'colbert report', 'say no', 'asian food', 'orpheum theater', 'twenty five lusk', 'nyer', 'church subway', 'science pass card', 'geary st', 'playstation', 'franzen', 'fattoush', 'silicon valley sleeps', 'strictly bluegrass fest', 'newly remodel cozy', 'outdoor patio easy', 'request for', 'welcome long', 'filtered', 'missionbay park', 'yep', 'animal care', 'sunflower', 'sorg', 'brian tracy', 'plot', 'bartlett sf', 'classic loft', 'larkin st', 'double bed suite', 'patio only', 'pluggedin', 'freshly remodeled unit cherry hardwood floors lots', 'bridgette', 'billionaire', 'dubuce triangle', 'emarcadero', 'uc hastings', 'city hills', 'indie', 'palo alto california', 'powell hyde', 'nearest', 'ingleside district', 'north potrero hill', 'events fellow', 'tonic', 'nashville', 'embardero', 'daly city ca', 'correct', 'xmas', 'river house', 'mcclaren park', 'fiji', 'hartford', 'northwestern germany', 'high speed wireless', 'wknd', 'voices', 'hdmi cable', 'glassware', 'design center', 'tech shuttle stops', 'mission impossible', 'castro streets', 'watch out', 'nouveau', 'historic union', 'edwardian san francisco', 'tyco events', 'reservations for', 'dropbox hq', 'embarcadero street', 'golden gate park beach judahlicious public transportation noriega', 'arthaven', 'vitalii', 'hot plate', 'it services', 'yield wine', 'beautiful remodeled large flat', 'uxdi', 'papito', 'david tao', 'louis vuitton', 'howard johnsons', 'ivy league', 'fillmore concert hall', 'dream force', 'new spot', 'bequem', 'montgomery bart', 'allergy', 'health tech', 'tracy', 'san francsico', 'hog island oyster co', 'dramatic sf bay', 'charlie parker', 'ggb', 'kitchen no', 'church st. valencia street', 'great japanese', 'karmann ghia', 'blowfish sushi', 'polaroid photography', 'broad city', 'cubix yerba buena', 'salvation army', 'alps', 'alden', 'high speed wireless internet', 'ofcasa fantast', 'private bathroom awesome', 'fvf book', 'states aspen colorado', 'common rooftop deck', 'markethall', 'ipsofacto', 'fayes video', 'fast at', 'outside lands quiet', 'both union', 'chinatownferrybuildingnobhillmoscone', 'vanguard', 'faithless', 'please go', 'pennsylvania ave', 'transitional', 'ashbury any', 'rosie', 'macgruber', 'ukraine', 'valid id', 'pierce', 'night parties', 'mexico city', 'ocean beach dis', 'haight ashbury victorian', 'happy yellow house', 'hastings', 'lisha', 'tel aviv', 'palahniuk', 'jefferson airplane', 'google', 'san francisco gorgeous', 'mission rock resort', 'cozy king', 'sinaia', 'grammy', 'aliens', 'deyoung art museum', 'soundcloud', 'standard sf lease', 'value across', 'erasmus', 'premium channels', 'biggie', 'daft punk', 'kuerig', 'queen air mattress', 'pivot tables', 'folsom streets', 'cozy modern', 'samovar', 'mosaic steps', 'akon', 'gordon', 'fitbit', 'soma walk score', 'hartford street', 'geneva avenue', 'hilton', 'western sf', 'expandable', 'driveway or', 'social kitchen', 'haiti', 'bart metro very', 'rent price', 'uc hastings college', 'bijou', 'laundry service', 'cavern', 'jane roberts', 'larkin street', 'sally', 'spice jar', 'chinese art museum', 'keetsa', 'retail stores', 'san francisco soul food restaurant', 'dillon', 'lightness of being', 'ashwood lane', 'upper market san francisco', 'east bay views', 'corp housing', 'farms markets', 'microwave coffee maker', 'copier', 'yellow house', 'haight st wake', 'andy goldsworthy', 'rock resort', 'courtland', 'desert solitaire', 'safe way', 'sf tennis club', 'safe marina neighborhood', 'famous sf', 'lonesome dove', 'ocean beach bungalow', 'oceanfront views', 'northstar', 'aziz', 'westlake district', 'rufino', 'bistro central parc', 'squaw', 'houseguest reference', 'louis armstrong', 'old miraloma', 'peets', 'san francisco didgeridog bed', 'talmud', 'shelley trew', 'nightly rate', 'sixth course', 'balboa cafe', 'karishma', 'pittsburgh steelers', 'segway rental', 'dreamworks', 'muni to downtown', 'san francisco panoramic', 'carryout', 'geary street', 'naia', 'ebba', 'bunny island', 'lord george', 'downtown walk', 'nord', 'cheapest', 'northpoint', 'carmel', 'second private bedroom', 'gordo', 'pacific heights.clean', 'terra cotta', 'howdy', 'yield wine bar', 'jemina', 'san franciscos richmond', 'fulton street', 'millennium', 'blue bloods', 'eq3', 'laurbana', 'glen park bart station mission district bus stops daly city down town potrero hill', 'national landmark', 'delays', 'chipotle', 'punta cana', 'clayton street manor', 'robby', 'beautiful victorion home', 'panoramic views top floor', 'please put', 'wifi access', 'beep', 'canterbury resort', 'american history', 'wfi', 'kentucky', 'vinyasa', 'unbearable', 'coachella', 'fridays', 'russians', 'hotel calista', 'sweet suite', 'cork', 'hbogo', 'rental rules', 'shady', 'common roof', 'ucsb', 'gym rooftop', 'mission bay ucsf', 'uc hasting', 'uc merced', 'late check out', 'upper noe', 'months minimum rental', 'garaje', 'carnival', 'cross road cafe', 'jazz club', 'mezzanine', 'parnassus heights', 'pleasant stay', 'javaone', 'presidio park gate', 'central park cafe', 'national public radio', 'california college', 'telluride', 'science academy', 'saru sushi', 'ocean beach bus', 'four barrell coffee', 'holistic health', 'excelisor', 'full cable', 'silicon valley shuttle', 'sf sleeps', 'blue bottle cafe', 'bamboo floors', 'see photos', 'wwdc', 'papalote mexican', 'vlogbrothers', 'taxi if', 'yost theater', 'party place', 'smokestack', 'west portal village', 'attn', 'personal reality', 'sf certificate', 'walden', 'kitchen bar', 'wifi private', 'trains go', 'ucsf children', 'bean bag caf', 'airbnb calendar', 'upper market street', 'sweet treats', 'exploratorium thursdays', 'goldilocks', 'furnished beach house', 'cambridge', 'alain', 'castro center location', 'fiddler', 'crunch', 'victorian castro', 'mainland china', 'velvet cantina', 'taylor swift', 'central park', 'rentals summer', 'precita park garage', 'recharge', 'hightech', 'sleeper sofas', 'mascone center', 'durian', 'shuttle stops', 'toffee crunch', 'security deposit rules', 'ivy street', 'tostador microwey', 'fairbanks', 'prodigy', 'filtered water', 'sigmund stern recreation grove', 'needless', 'crepe house iii', 'bluescreen room', 'personal assistant', 'piccino cafe', 'product engineer', 'silence', 'neighborhood bakeho', 'garbage disposal', 'cheese plus', 'original philz', 'derek walcott', 'bfa', 'sofia', 'new washer', 'fishing', 'wisconsin st', 'kitchen shared', 'bedroom full size bed', 'marian', 'liquor store', 'location great', 'bradbury', 'naval officer', 'nfl experience', 'safeway supermarket', 'waller', 'food truck', 'bike sto', 'sf main library', 'masters degree', 'airstream', 'hdmi', 'stowe', 'bayshore', 'sf shopping center', 'fenced', 'xfinity', 'cafe chez maman papito live sushi sunflower yield wine bar chocolate lab', 'cardio', 'scrubs', 'golden gate bridge located', 'esthetician', 'kilimanjaro', 'gospel', 'hbog', 'san francisco architecture', 'spreckels lake', 'four square coffee', 'flight attendant', 'vibrant valencia street', 'food co', 'barrel head brewhouse', 'suite retreat', 'analog', 'us navy', 'brannons', 'mission district sun', 'ucsf shuttle', 'bronco', 'bike storage', 'old town santa ana', 'buzzer', 'bdrs', 'michael jackson', 'swift', 'courtland avenue', 'anja', 'victorian haight street', 'mastersuite', 'recherche', 'parking ca', 'main library', 'horace mann', 'thuy', 'entertainment ctr', 'slava', 'ingleside san francisco', 'panther', 'california rental house rules', 'castro heights', 'monkeybrains', 'empire', 'asian fusion', 'lake como', 'asha', 'soma powell st cable cars', 'newest luxury living skyscraper']\n"
     ]
    }
   ],
   "source": [
    "keys = sorted(unique_entity_list.items(), key=operator.itemgetter(1))\n",
    "sorted_keys = [key[0] for key in keys]\n",
    "least_common_1000 = sorted_keys[:1000]\n",
    "print least_common_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509898\n",
      "397754\n"
     ]
    }
   ],
   "source": [
    "#loading dependent text\n",
    "lines = []\n",
    "filtered_spaces = []\n",
    "for line in open('text_test_deps.csv').readlines():\n",
    "    temp_str = line.strip('\\r\\n')\n",
    "    #there are empty spaces in text\n",
    "    if temp_str != \"\":\n",
    "        lines.append(temp_str)\n",
    "        filtered_spaces.append(temp_str)\n",
    "    else:\n",
    "    #fills the empty spaces\n",
    "        lines.append(\"empty_row\")\n",
    "        \n",
    "for line in open('text_train_deps.csv').readlines():\n",
    "    temp_str = line.strip('\\r\\n')\n",
    "    #there are empty spaces in text\n",
    "    if temp_str != \"\":\n",
    "        lines.append(temp_str)\n",
    "        filtered_spaces.append(temp_str)\n",
    "    else:\n",
    "    #fills the empty spaces\n",
    "        lines.append(\"empty_row\")\n",
    "        \n",
    "\n",
    "print len(lines)\n",
    "print len(filtered_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Taylor Swift\n"
     ]
    }
   ],
   "source": [
    "print lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is only the test set\n",
    "#this will use nltk's built in sentiment score\n",
    "#more info on this: http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "#only digested 8 lines to demonstrate. Will continue if needed.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "tfidf_sentiment = []\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "for line in lines:\n",
    "    word_count = 0\n",
    "    term_freq = {}\n",
    "    frequency = []\n",
    "    senten = nltk.sent_tokenize(line.decode('ascii', 'ignore'))\n",
    "    token_sent = [nltk.word_tokenize(sentence) for sentence in senten]    \n",
    "    tagged = [nltk.pos_tag(sentence) for sentence in token_sent]\n",
    "    chunked = nltk.ne_chunk_sents(tagged, binary=True)\n",
    "    \n",
    "    for tree in chunked:\n",
    "        frequency.extend(extract_entity_names(tree))\n",
    "        #gets chunked vocab count\n",
    "        for each in tree:\n",
    "            word_count += 1\n",
    "    \n",
    "    \n",
    "    for each in frequency:        \n",
    "        for word in least_common_5000:\n",
    "            if word == each.lower():\n",
    "                if word in term_freq:\n",
    "                    term_freq[word] += 1\n",
    "                else:\n",
    "                    term_freq[word] = 1 \n",
    "            \n",
    "\n",
    "\n",
    "    if len(term_freq) > 0:\n",
    "        for key in term_freq.keys():\n",
    "            tf = float(term_freq[key])/word_count\n",
    "            idf = math.log(word_count/(1+float(term_freq[key])))\n",
    "            tf_idf = tf*idf\n",
    "        ss = sid.polarity_scores(line)\n",
    "        tfidf_sentiment.append([[key,tf_idf], ss])\n",
    "    else:\n",
    "        ss = sid.polarity_scores(line)\n",
    "        tfidf_sentiment.append([[\"no_keys\", 0.0], ss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['dallas', 0.019436619216149344], {'neg': 0.0, 'neu': 0.959, 'pos': 0.041, 'compound': 0.7767}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.6369}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['taylor swift', 0.0], {'neg': 0.0, 'neu': 0.357, 'pos': 0.643, 'compound': 0.2023}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['cole street', 0.01913139252240148], {'neg': 0.048, 'neu': 0.795, 'pos': 0.157, 'compound': 0.9834}]\n",
      "[['no_keys', 0.0], {'neg': 0.024, 'neu': 0.824, 'pos': 0.152, 'compound': 0.9961}]\n",
      "[['no_keys', 0.0], {'neg': 0.011, 'neu': 0.597, 'pos': 0.392, 'compound': 0.9987}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.727, 'pos': 0.273, 'compound': 0.908}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.093, 'neu': 0.567, 'pos': 0.34, 'compound': 0.9893}]\n",
      "[['sharp', 0.01720857961090826], {'neg': 0.0, 'neu': 0.835, 'pos': 0.165, 'compound': 0.9918}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.023, 'neu': 0.74, 'pos': 0.238, 'compound': 0.9524}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.9716}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.849, 'pos': 0.151, 'compound': 0.946}]\n",
      "[['no_keys', 0.0], {'neg': 0.02, 'neu': 0.944, 'pos': 0.036, 'compound': 0.6351}]\n",
      "[['no_keys', 0.0], {'neg': 0.028, 'neu': 0.861, 'pos': 0.11, 'compound': 0.9658}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.912, 'pos': 0.088, 'compound': 0.6808}]\n",
      "[['no_keys', 0.0], {'neg': 0.018, 'neu': 0.764, 'pos': 0.217, 'compound': 0.9978}]\n",
      "[['no_keys', 0.0], {'neg': 0.011, 'neu': 0.83, 'pos': 0.159, 'compound': 0.9953}]\n",
      "[['haight ashbury district', 0.040959017039468706], {'neg': 0.0, 'neu': 0.937, 'pos': 0.063, 'compound': 0.6597}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['haight ashbury district', 0.040959017039468706], {'neg': 0.0, 'neu': 0.694, 'pos': 0.306, 'compound': 0.9834}]\n",
      "[['no_keys', 0.0], {'neg': 0.016, 'neu': 0.867, 'pos': 0.116, 'compound': 0.9743}]\n",
      "[['dartmouth', 0.015859918172605708], {'neg': 0.005, 'neu': 0.878, 'pos': 0.117, 'compound': 0.987}]\n",
      "[['no_keys', 0.0], {'neg': 0.02, 'neu': 0.845, 'pos': 0.135, 'compound': 0.9834}]\n",
      "[['waller', 0.01383743805942607], {'neg': 0.012, 'neu': 0.831, 'pos': 0.158, 'compound': 0.9939}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.838, 'pos': 0.162, 'compound': 0.7845}]\n",
      "[['no_keys', 0.0], {'neg': 0.061, 'neu': 0.689, 'pos': 0.249, 'compound': 0.9861}]\n",
      "[['no_keys', 0.0], {'neg': 0.008, 'neu': 0.843, 'pos': 0.149, 'compound': 0.9927}]\n",
      "[['no_keys', 0.0], {'neg': 0.009, 'neu': 0.831, 'pos': 0.16, 'compound': 0.994}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.9359}]\n",
      "[['saint mary', 0.02128909901497189], {'neg': 0.021, 'neu': 0.846, 'pos': 0.133, 'compound': 0.9949}]\n",
      "[['west portal avenue', 0.009959809741036696], {'neg': 0.017, 'neu': 0.801, 'pos': 0.182, 'compound': 0.9977}]\n",
      "[['no_keys', 0.0], {'neg': 0.08, 'neu': 0.728, 'pos': 0.192, 'compound': 0.8481}]\n",
      "[['indoor parking', 0.02707397094846584], {'neg': 0.008, 'neu': 0.709, 'pos': 0.283, 'compound': 0.9985}]\n",
      "[['sf airport', 0.019752733591689726], {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.9852}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['sf airport', 0.02063013968853587], {'neg': 0.009, 'neu': 0.815, 'pos': 0.176, 'compound': 0.9868}]\n",
      "[['aero', 0.021835648759167783], {'neg': 0.027, 'neu': 0.82, 'pos': 0.153, 'compound': 0.9713}]\n",
      "[['no_keys', 0.0], {'neg': 0.132, 'neu': 0.617, 'pos': 0.251, 'compound': 0.4391}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.832, 'pos': 0.168, 'compound': 0.8655}]\n",
      "[['armed forces', 0.0849503145729253], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'compound': 0.4767}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.456, 'pos': 0.544, 'compound': 0.964}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.893, 'pos': 0.107, 'compound': 0.5719}]\n",
      "[['aero', 0.027790437100562823], {'neg': 0.023, 'neu': 0.801, 'pos': 0.176, 'compound': 0.954}]\n",
      "[['no_keys', 0.0], {'neg': 0.032, 'neu': 0.866, 'pos': 0.102, 'compound': 0.977}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.885, 'pos': 0.115, 'compound': 0.9661}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.3612}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.4588}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.743, 'pos': 0.257, 'compound': 0.5859}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.613, 'pos': 0.387, 'compound': 0.6486}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.5411}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.4588}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.6369}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.3164}]\n",
      "[['bryant', 0.020701200100411765], {'neg': 0.061, 'neu': 0.827, 'pos': 0.112, 'compound': 0.9118}]\n",
      "[['no_keys', 0.0], {'neg': 0.022, 'neu': 0.827, 'pos': 0.151, 'compound': 0.9962}]\n",
      "[['keetsa', 0.00941578519027933], {'neg': 0.026, 'neu': 0.827, 'pos': 0.147, 'compound': 0.9965}]\n",
      "[['kristoffer', 0.019313254949209206], {'neg': 0.026, 'neu': 0.771, 'pos': 0.203, 'compound': 0.9919}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.74, 'pos': 0.26, 'compound': 0.9827}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.9812}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.531, 'pos': 0.469, 'compound': 0.9607}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.4767}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['kristoffer', 0.03826778454048243], {'neg': 0.03, 'neu': 0.751, 'pos': 0.218, 'compound': 0.9329}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'compound': 0.9966}]\n",
      "[['no_keys', 0.0], {'neg': 0.031, 'neu': 0.847, 'pos': 0.121, 'compound': 0.9935}]\n",
      "[['no_keys', 0.0], {'neg': 0.019, 'neu': 0.782, 'pos': 0.199, 'compound': 0.9992}]\n",
      "[['no_keys', 0.0], {'neg': 0.01, 'neu': 0.802, 'pos': 0.188, 'compound': 0.9872}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'compound': 0.4939}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.8316}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.043, 'neu': 0.779, 'pos': 0.178, 'compound': 0.9864}]\n",
      "[['forest hill muni station', 0.017904086971296824], {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.9526}]\n",
      "[['no_keys', 0.0], {'neg': 0.044, 'neu': 0.831, 'pos': 0.125, 'compound': 0.9708}]\n",
      "[['no_keys', 0.0], {'neg': 0.005, 'neu': 0.852, 'pos': 0.143, 'compound': 0.9949}]\n",
      "[['indiana', 0.009252535963676116], {'neg': 0.026, 'neu': 0.873, 'pos': 0.101, 'compound': 0.9865}]\n",
      "[['ccsf', 0.01688247389785174], {'neg': 0.061, 'neu': 0.805, 'pos': 0.134, 'compound': 0.9686}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "for each in tfidf_sentiment[:100]:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is both train and test dependency together\n",
    "#this will use nltk's built in sentiment score\n",
    "#more info on this: http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "#only digested 8 lines to demonstrate. Will continue if needed.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "tfidf_sentiment = []\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "for line in lines:\n",
    "    word_count = 0\n",
    "    term_freq = {}\n",
    "    frequency = []\n",
    "    senten = nltk.sent_tokenize(line.decode('ascii', 'ignore'))\n",
    "    token_sent = [nltk.word_tokenize(sentence) for sentence in senten]    \n",
    "    tagged = [nltk.pos_tag(sentence) for sentence in token_sent]\n",
    "    chunked = nltk.ne_chunk_sents(tagged, binary=True)\n",
    "    \n",
    "    for tree in chunked:\n",
    "        frequency.extend(extract_entity_names(tree))\n",
    "        #gets chunked vocab count\n",
    "        for each in tree:\n",
    "            word_count += 1\n",
    "    \n",
    "    \n",
    "    for each in frequency:        \n",
    "        for word in least_common_5000:\n",
    "            if word == each.lower():\n",
    "                if word in term_freq:\n",
    "                    term_freq[word] += 1\n",
    "                else:\n",
    "                    term_freq[word] = 1 \n",
    "            \n",
    "\n",
    "\n",
    "    if len(term_freq) > 0:\n",
    "        for key in term_freq.keys():\n",
    "            tf = float(term_freq[key])/word_count\n",
    "            idf = math.log(word_count/(1+float(term_freq[key])))\n",
    "            tf_idf = tf*idf\n",
    "        ss = sid.polarity_scores(line)\n",
    "        tfidf_sentiment.append([[key,tf_idf], ss])\n",
    "    else:\n",
    "        ss = sid.polarity_scores(line)\n",
    "        tfidf_sentiment.append([[\"no_keys\", 0.0], ss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['thx', 0.016525922765941697], {'neg': 0.031, 'neu': 0.863, 'pos': 0.106, 'compound': 0.9508}]\n",
      "[['no_keys', 0.0], {'neg': 0.041, 'neu': 0.83, 'pos': 0.129, 'compound': 0.9808}]\n",
      "[['no_keys', 0.0], {'neg': 0.013, 'neu': 0.909, 'pos': 0.077, 'compound': 0.8885}]\n",
      "[['no_keys', 0.0], {'neg': 0.056, 'neu': 0.808, 'pos': 0.136, 'compound': 0.9826}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.914, 'pos': 0.086, 'compound': 0.9741}]\n",
      "[['no_keys', 0.0], {'neg': 0.007, 'neu': 0.867, 'pos': 0.126, 'compound': 0.9846}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['sf giants', 0.009845604974325923], {'neg': 0.042, 'neu': 0.786, 'pos': 0.172, 'compound': 0.9969}]\n",
      "[['no_keys', 0.0], {'neg': 0.027, 'neu': 0.808, 'pos': 0.165, 'compound': 0.9967}]\n",
      "[['no_keys', 0.0], {'neg': 0.022, 'neu': 0.852, 'pos': 0.126, 'compound': 0.9682}]\n",
      "[['no_keys', 0.0], {'neg': 0.009, 'neu': 0.766, 'pos': 0.225, 'compound': 0.9933}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.593, 'pos': 0.407, 'compound': 0.9595}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.158, 'neu': 0.842, 'pos': 0.0, 'compound': -0.3875}]\n",
      "[['no_keys', 0.0], {'neg': 0.11, 'neu': 0.755, 'pos': 0.134, 'compound': 0.658}]\n",
      "[['historic carriage house', 0.011535991858934875], {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.9974}]\n",
      "[['location located', 0.012682128278379307], {'neg': 0.036, 'neu': 0.768, 'pos': 0.196, 'compound': 0.997}]\n",
      "[['panhandle parks', 0.08354093687234569], {'neg': 0.122, 'neu': 0.878, 'pos': 0.0, 'compound': -0.8225}]\n",
      "[['no_keys', 0.0], {'neg': 0.024, 'neu': 0.859, 'pos': 0.117, 'compound': 0.9816}]\n",
      "[['no_keys', 0.0], {'neg': 0.018, 'neu': 0.888, 'pos': 0.094, 'compound': 0.9758}]\n",
      "[['denis', 0.014655299933068983], {'neg': 0.056, 'neu': 0.746, 'pos': 0.198, 'compound': 0.9932}]\n",
      "[['melissa', 0.011321653011087708], {'neg': 0.013, 'neu': 0.792, 'pos': 0.195, 'compound': 0.9975}]\n",
      "[['no_keys', 0.0], {'neg': 0.005, 'neu': 0.828, 'pos': 0.167, 'compound': 0.998}]\n",
      "[['no_keys', 0.0], {'neg': 0.028, 'neu': 0.803, 'pos': 0.168, 'compound': 0.9814}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.029, 'neu': 0.808, 'pos': 0.164, 'compound': 0.9892}]\n",
      "[['no_keys', 0.0], {'neg': 0.025, 'neu': 0.83, 'pos': 0.145, 'compound': 0.8834}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['soma private room', 0.02999824587850897], {'neg': 0.058, 'neu': 0.81, 'pos': 0.132, 'compound': 0.9643}]\n",
      "[['haiti', 0.019624942979529724], {'neg': 0.017, 'neu': 0.725, 'pos': 0.258, 'compound': 0.9958}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.01, 'neu': 0.774, 'pos': 0.216, 'compound': 0.9896}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.9954}]\n",
      "[['driveway parking', 0.009264875754644025], {'neg': 0.037, 'neu': 0.87, 'pos': 0.093, 'compound': 0.983}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.9899}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.034, 'neu': 0.745, 'pos': 0.22, 'compound': 0.9098}]\n",
      "[['no_keys', 0.0], {'neg': 0.028, 'neu': 0.842, 'pos': 0.13, 'compound': 0.893}]\n",
      "[['young museum easy', 0.021064727103995956], {'neg': 0.01, 'neu': 0.889, 'pos': 0.101, 'compound': 0.9485}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['young museum easy', 0.029386708501016375], {'neg': 0.038, 'neu': 0.844, 'pos': 0.118, 'compound': 0.9229}]\n",
      "[['no_keys', 0.0], {'neg': 0.007, 'neu': 0.865, 'pos': 0.128, 'compound': 0.9867}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['genentech', 0.007973318143675788], {'neg': 0.043, 'neu': 0.826, 'pos': 0.131, 'compound': 0.9959}]\n",
      "[['housekeeper', 0.012561210380957095], {'neg': 0.01, 'neu': 0.723, 'pos': 0.268, 'compound': 0.9987}]\n",
      "[['no_keys', 0.0], {'neg': 0.018, 'neu': 0.847, 'pos': 0.135, 'compound': 0.9785}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['suv', 0.020147312069937494], {'neg': 0.056, 'neu': 0.882, 'pos': 0.062, 'compound': 0.6151}]\n",
      "[['swank and', 0.020697350447767063], {'neg': 0.051, 'neu': 0.79, 'pos': 0.159, 'compound': 0.9938}]\n",
      "[['no_keys', 0.0], {'neg': 0.049, 'neu': 0.898, 'pos': 0.053, 'compound': 0.1027}]\n",
      "[['hottub', 0.031025318686864534], {'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.9964}]\n",
      "[['guerrero', 0.014555571770483385], {'neg': 0.079, 'neu': 0.787, 'pos': 0.134, 'compound': 0.9569}]\n",
      "[['rose garden', 0.029697066465636062], {'neg': 0.026, 'neu': 0.769, 'pos': 0.205, 'compound': 0.993}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.9504}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.837, 'pos': 0.163, 'compound': 0.9844}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['san franciscos mission district', 0.008819690773096902], {'neg': 0.037, 'neu': 0.753, 'pos': 0.21, 'compound': 0.9991}]\n",
      "[['no_keys', 0.0], {'neg': 0.019, 'neu': 0.695, 'pos': 0.286, 'compound': 0.9954}]\n",
      "[['no_keys', 0.0], {'neg': 0.007, 'neu': 0.874, 'pos': 0.119, 'compound': 0.9835}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.9325}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['google', 0.021064727103995956], {'neg': 0.008, 'neu': 0.872, 'pos': 0.12, 'compound': 0.9708}]\n",
      "[['no_keys', 0.0], {'neg': 0.041, 'neu': 0.864, 'pos': 0.095, 'compound': 0.8507}]\n",
      "[['no_keys', 0.0], {'neg': 0.056, 'neu': 0.859, 'pos': 0.085, 'compound': 0.7284}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.041, 'neu': 0.765, 'pos': 0.194, 'compound': 0.9803}]\n",
      "[['jcc', 0.012123703523453181], {'neg': 0.044, 'neu': 0.743, 'pos': 0.213, 'compound': 0.9971}]\n",
      "[['no_keys', 0.0], {'neg': 0.009, 'neu': 0.77, 'pos': 0.221, 'compound': 0.998}]\n",
      "[['tuft', 0.008875609480623785], {'neg': 0.034, 'neu': 0.795, 'pos': 0.171, 'compound': 0.9978}]\n",
      "[['no_keys', 0.0], {'neg': 0.015, 'neu': 0.778, 'pos': 0.207, 'compound': 0.9936}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.026, 'neu': 0.77, 'pos': 0.204, 'compound': 0.9545}]\n",
      "[['no_keys', 0.0], {'neg': 0.033, 'neu': 0.826, 'pos': 0.141, 'compound': 0.9966}]\n",
      "[['adjust', 0.021064727103995956], {'neg': 0.007, 'neu': 0.775, 'pos': 0.219, 'compound': 0.992}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.631, 'pos': 0.369, 'compound': 0.6249}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['atms', 0.015859918172605708], {'neg': 0.02, 'neu': 0.818, 'pos': 0.161, 'compound': 0.9911}]\n",
      "[['ghia', 0.01593995032144727], {'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.9955}]\n",
      "[['no_keys', 0.0], {'neg': 0.066, 'neu': 0.934, 'pos': 0.0, 'compound': -0.6808}]\n",
      "[['golden gate parks', 0.02021476535790536], {'neg': 0.0, 'neu': 0.729, 'pos': 0.271, 'compound': 0.9969}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'compound': 0.4215}]\n",
      "[['golden gate parks', 0.020772800212256557], {'neg': 0.014, 'neu': 0.774, 'pos': 0.212, 'compound': 0.9924}]\n",
      "[['bottom flat', 0.020701200100411765], {'neg': 0.018, 'neu': 0.846, 'pos': 0.136, 'compound': 0.9783}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.056, 'neu': 0.775, 'pos': 0.169, 'compound': 0.8126}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 0.812, 'pos': 0.188, 'compound': 0.6757}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "[['no_keys', 0.0], {'neg': 0.034, 'neu': 0.831, 'pos': 0.135, 'compound': 0.9349}]\n",
      "[['hdmi', 0.009240231725239184], {'neg': 0.024, 'neu': 0.85, 'pos': 0.127, 'compound': 0.9956}]\n",
      "[['no_keys', 0.0], {'neg': 0.008, 'neu': 0.786, 'pos': 0.205, 'compound': 0.9948}]\n"
     ]
    }
   ],
   "source": [
    "for each in tfidf_sentiment[509800:509898]:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('tfidf_sentiment.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(tfidf_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
