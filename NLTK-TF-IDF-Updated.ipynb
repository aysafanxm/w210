{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119615\n",
      "119615\n"
     ]
    }
   ],
   "source": [
    "#import csv as pandas array\n",
    "#text_train_deps = pd.read_csv(\"text_train_deps.csv\", encoding='utf8')\n",
    "text_train_deps = pd.read_csv(\"./data/fin_text_train_deps.csv\", encoding='utf8', dtype=str)\n",
    "\n",
    "\n",
    "text_train_deps.head(5)\n",
    "\n",
    "train_sentences = []\n",
    "#combines all columns into 1 string\n",
    "n = len(text_train_deps)\n",
    "for row in text_train_deps['description'] + ';' + text_train_deps['host_about'] + ';' + \\\n",
    "text_train_deps['house_rules'] + ';' + text_train_deps['neighborhood_overview'] + ';' + \\\n",
    "text_train_deps['notes'] + ';' + text_train_deps['summary']:\n",
    "    train_sentences.append(str(row))\n",
    "print len(train_sentences)\n",
    "print len(text_train_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco Station AT T Park Safeway;None;None;None;None;San Francisco Station AT T Park Safeway\n",
      "a Private Bedroom in a classic Victorian one block from famous 24th Street in Noe Valley easy to access the J Church Municipal Railway and the BART system Best Location you will find in the City We are steps from 24th Street in the heart of Noe Valley When family is not staying we would like to offer up our home to guests traveling through San Francisco the most beautiful city in the world We suggest two people because we are planning to use rent out other spaces within our home however if you have a more than than four we can make arrangements to rent the entire top floor which is equivalent to a flat Access to the dining room kitchen and bathroom The downstairs is a live work space that is occupied by the owner I d love to hear about your journeys and may be available during your stay I can assist with suggestions local and otherwise to enhance your journey Geographically Noe Valley really does feel like a little town The hillsides of Diamond Heights and Tw;San Francisco CA resident Midwest born I have children who accompany during travel It is with the heart that one sees rightly what is essential is invisible to the eye;We are hoping to encourage families students or those coming to SF for work or vacation Pets are not allowed Smoking is also prohibited as reiterated below Extra guests Visitors are allowed however overnight guests are restricted typically to the number in the listing typically two 2 Additional guests must be approved and need to be added to the reservation Call or write with any questions or book your stay with the correct number of guests Smoking is prohibited in the home Feel free to smoke outdoors not on premise though And please I beg that you don t disturb our neighbors Off limit areas Please respect the other people and parts of the home that are not part of your accommodation Please steer clear of areas that are part of the owner s area business or bedroom s Also if other guests or members of our family are present please be respectful and don t invade upon other guest s space Eating areas Due to pest control and cleanliness we ask that you;Geographically Noe Valley really does feel like a little town The hillsides of Diamond Heights and Twin Peaks make a picturesque backdrop day or night for the straight rows of businesses along 24th Street It s a domestic dream village where Jim Dear and Darling from Disney s Lady and the Tramp would live The row houses in Noe Valley were built beginning in the late 1800s through the early 1900s and were meant for working class families who couldn t afford to live in the more upscale areas of the city;You will be hard pressed to find a better location in San Francisco that is in a neighborhood STR PHONE NUMBER HIDDEN;a Private Bedroom in a classic Victorian one block from famous 24th Street in Noe Valley easy to access the J Church Municipal Railway and the BART system Best Location you will find in the City\n"
     ]
    }
   ],
   "source": [
    "for each in train_sentences[:2]:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating entity features\n",
    "\n",
    "corpus = train_sentences\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        # print t.label()\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "#getting tf and word count\n",
    "document_entity_features = []\n",
    "\n",
    "for row in corpus:\n",
    "    sentences = nltk.sent_tokenize(row.decode('ascii', 'ignore'))\n",
    "    # print '\\nsentences:'\n",
    "    # print sentences\n",
    "    \n",
    "    token_sent = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    # print '\\ntoken_sent:'\n",
    "    # print token_sent\n",
    "    \n",
    "    tagged = [nltk.pos_tag(sentence) for sentence in token_sent]\n",
    "    # print '\\ntagged:'\n",
    "    # print tagged\n",
    "    \n",
    "    chunked = nltk.ne_chunk_sents(tagged, binary=True)\n",
    "    # print '\\nchunked:'\n",
    "    # print list(chunked)\n",
    "    \n",
    "    for tree in chunked:\n",
    "        document_entity_features.append(extract_entity_names(tree))\n",
    "    # print '\\ndocument_entity_features:'\n",
    "    # print document_entity_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accumulate the \"feature appearance\" in document counts and feature word counts\n",
    "document_count = {}\n",
    "common_feature_words = {}\n",
    "for doc in document_entity_features:\n",
    "    current_document_count = {}\n",
    "    for word in doc:\n",
    "        if word.lower() not in stop_words:\n",
    "            #gets the most common feature words\n",
    "            if word.lower() not in common_feature_words:\n",
    "                common_feature_words[word.lower()] = 1\n",
    "            else:\n",
    "                common_feature_words[word.lower()] += 1\n",
    "            \n",
    "    #accumulates the appearance of word in documents\n",
    "            if word.lower() not in current_document_count:\n",
    "                current_document_count[word.lower()] = 1\n",
    "\n",
    "    for key in current_document_count.keys():\n",
    "        if key not in document_count:\n",
    "            document_count[key] = current_document_count[key]\n",
    "        elif key in document_count:\n",
    "            document_count[key] += current_document_count[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls the least common 10000 words\n",
    "#set the number of words\n",
    "n = 10000\n",
    "\n",
    "print len(common_feature_words)\n",
    "keys = sorted(common_feature_words.items(), key=operator.itemgetter(1))\n",
    "sorted_keys = [str(key[0]) for key in keys]\n",
    "least_common_keys = sorted_keys[:n]\n",
    "#print common_feature_words\n",
    "least_common_entities = {}\n",
    "for key in least_common_keys:\n",
    "    least_common_entities[key] = common_feature_words[key]\n",
    "\n",
    "print len(least_common_entities)\n",
    "print least_common_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "#getting tf and word count\n",
    "total_word_count = 0\n",
    "tfidf_sentiment = []\n",
    "#for pandas export purposes\n",
    "tfidf = []\n",
    "sentiment = []\n",
    "\n",
    "for row in corpus:\n",
    "    current_entities = []\n",
    "    temp_tfidf = []\n",
    "    sentences = nltk.sent_tokenize(row.decode('ascii', 'ignore'))\n",
    "    token_sent = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    tagged = [nltk.pos_tag(sentence) for sentence in token_sent]\n",
    "    chunked = nltk.ne_chunk_sents(tagged, binary=True)\n",
    "    \n",
    "    for tree in chunked:\n",
    "        current_entities.extend(extract_entity_names(tree))\n",
    "    #gets chunked word count\n",
    "    for word in sentence:\n",
    "        total_word_count += 1   \n",
    "        \n",
    "    if len(current_entities) > 0:\n",
    "        \n",
    "        #counts the amount of times the word appears in the sentence\n",
    "        current_word_count = {}\n",
    "        for word in current_entities:\n",
    "            lower_word = word.lower()\n",
    "            if lower_word in least_common_entities:\n",
    "                if lower_word not in current_word_count:\n",
    "                    current_word_count[lower_word] = 1\n",
    "                elif lower_word in current_word_count:\n",
    "                    current_word_count[lower_word] += 1\n",
    "        if len(current_word_count) > 0:    \n",
    "            \n",
    "            for word in current_word_count.keys():\n",
    "                tf = float(current_word_count[word])/total_word_count\n",
    "                idf = math.log(len(corpus)/(1+float(document_count[word])))\n",
    "                tf_idf = tf*idf\n",
    "                temp_tfidf.append([word,tf_idf])\n",
    "        else:\n",
    "                temp_tfidf.append([\"no_keys\", str(0.0)])\n",
    "    else:\n",
    "        temp_tfidf.append([\"no_keys\", str(0.0)])\n",
    "        \n",
    "    ss = sid.polarity_scores(row)\n",
    "    tfidf_sentiment.append([temp_tfidf, ss])    \n",
    "    \n",
    "    #for exporting pandas purposes\n",
    "    tfidf.append(temp_tfidf)\n",
    "    sentiment.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(tfidf_sentiment)\n",
    "#for each in tfidf_sentiment:\n",
    "#    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in tfidf_sentiment[:1000]:\n",
    "    print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "pd_tfidf_sentiment = DataFrame.from_records(tfidf_sentiment)\n",
    "pd_tfidf_sentiment.columns = [\"keys\", \"sentiment\"]\n",
    "pd_tfidf_sentiment.head(5)\n",
    "\n",
    "pd_tfidf_sentiment.to_csv('pd_tfidf_sentiment.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"pd_tfidf_sentiment.csv\")\n",
    "test.head(5)\n",
    "print len(text_train_deps)\n",
    "print len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
